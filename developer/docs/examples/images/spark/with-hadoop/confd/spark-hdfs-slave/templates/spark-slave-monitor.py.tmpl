import sys
import urllib2
import json
import re

SPARK_DFS_ROLE_MASTER = "spark-hdfs-master"
SPARK_DFS_ROLE_SLAVE = "spark-hdfs-slave"
SPARK_ROLE_MASTER = "spark-master"
SPARK_ROLE_WORKER = "spark-worker"

def parse_spark_dfs_stat(role, hjson, stats):    
    if "beans" not in hjson:
        return
    if role == SPARK_DFS_ROLE_MASTER:
        for sub_dict in hjson["beans"]:
            if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=NameNode,name=NameNodeActivity":
                stats["FilesCreated"] = sub_dict["FilesCreated"]
                stats["FilesAppended"] = sub_dict["FilesAppended"]
		stats["FilesRenamed"] = sub_dict["FilesRenamed"]
                stats["FilesDeleted"] = sub_dict["FilesDeleted"]
	    if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=NameNode,name=FSNamesystem":
		stats["CapacityTotalGB"] = sub_dict["CapacityTotalGB"]
		stats["UsedGB"] = sub_dict["CapacityUsedGB"]
		stats["RemainingGB"] = sub_dict["CapacityRemainingGB"]
		stats["FilesTotal"] = sub_dict["FilesTotal"]
	    if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=NameNode,name=FSNamesystemState":
		stats["LiveNodes"] = sub_dict["NumLiveDataNodes"]
		stats["DeadNodes"] = sub_dict["NumDeadDataNodes"]
		stats["DecomLiveNodes"] = sub_dict["NumDecomLiveDataNodes"]
		stats["DecomDeadNodes"] = sub_dict["NumDecomDeadDataNodes"]
		stats["DecommissioningNodes"] = sub_dict["NumDecommissioningDataNodes"]
		stats["StaleNodes"] = sub_dict["NumStaleDataNodes"]
		stats["StaleStorages"] = sub_dict["NumStaleStorages"]
            if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=NameNode,name=NameNodeInfo":
                stats["PercentUsed"] = sub_dict["PercentUsed"]
		stats["PercentRemaining"] = sub_dict["PercentRemaining"]                
    elif role == SPARK_DFS_ROLE_SLAVE:
	for sub_dict in hjson["beans"]:
	    if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=DataNode,name=FSDatasetState-null":
		stats["RemainingGB"] = sub_dict["Remaining"] / (1024*1024*1024)
		stats["DfsUsedGB"] = sub_dict["DfsUsed"] / (1024*1024*1024)
	    if "name" in sub_dict and re.match(r'Hadoop:service=DataNode,name=DataNodeActivity-.*', sub_dict["name"]):
		stats["BlocksWritten"] = sub_dict["BlocksWritten"]
		stats["BlocksRead"] = sub_dict["BlocksRead"]
	    if "name" in sub_dict and sub_dict["name"] == "Hadoop:service=DataNode,name=JvmMetrics":
		stats["GcCount"] = sub_dict["GcCount"]
		stats["GcNumWarnThresholdExceeded"] = sub_dict["GcNumWarnThresholdExceeded"]

def parse_spark_stat(role, hjson, stats):    
    if "gauges" not in hjson:
        return	
    if role == SPARK_ROLE_MASTER:
	stats["WorkersTotal"] = hjson["gauges"]["master.workers"]["value"]
	stats["WorkersAlive"] = hjson["gauges"]["master.aliveWorkers"]["value"]
	stats["Running"] = hjson["gauges"]["master.apps"]["value"]
	stats["Waiting"] = hjson["gauges"]["master.waitingApps"]["value"]
    elif role == SPARK_ROLE_WORKER:
	stats["CoresFree"] = hjson["gauges"]["worker.coresFree"]["value"]
	stats["CoresUsed"] = hjson["gauges"]["worker.coresUsed"]["value"]
	stats["Executors"] = hjson["gauges"]["worker.executors"]["value"]
	stats["MemFreeMB"] = hjson["gauges"]["worker.memFree_MB"]["value"]
	stats["MemUsedMB"] = hjson["gauges"]["worker.memUsed_MB"]["value"]

if __name__ == '__main__':
    ip = '{{getv "/host/ip"}}'
    stats = {}
	
    role = SPARK_DFS_ROLE_SLAVE
    port = "50075"
    res = urllib2.urlopen("http://%s:%s/jmx?qry=Hadoop:service=DataNode,name=*" % (ip, port))
    pjson = json.loads(res.read())
    parse_spark_dfs_stat(role, pjson, stats)

    role = SPARK_ROLE_WORKER
    port = "8081"
    res = urllib2.urlopen("http://%s:%s/metrics/json/" % (ip, port))
    pjson = json.loads(res.read())
    parse_spark_stat(role, pjson, stats)	
    print json.dumps(stats)
